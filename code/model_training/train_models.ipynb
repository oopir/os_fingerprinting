{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy             as np\n",
    "import pandas            as pd\n",
    "import tensorflow        as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run preprocess_data.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.metrics         import top_k_accuracy_score, make_scorer\n",
    "\n",
    "\n",
    "def plot_learning_curve(estimator, title, X, y, train_sizes=np.linspace(0.1, 1.0, 5)):\n",
    "    # define & run cross-validation \n",
    "    cv = ShuffleSplit(n_splits=5, test_size=0.2, random_state=0)\n",
    "    top_2_scorer = make_scorer(top_k_accuracy_score, needs_proba=True, k=2)\n",
    "\n",
    "    train_sizes, train_scores, test_scores = \\\n",
    "        learning_curve(estimator, X, y, scoring=top_2_scorer, cv=cv, n_jobs=-1, train_sizes=np.linspace(0.1, 1.0, 5))\n",
    "\n",
    "    # Plot learning curve\n",
    "    plt.plot(train_sizes, np.mean(train_scores, axis=1), \"o-\", color=\"r\", label=\"train\")\n",
    "    plt.plot(train_sizes, np.mean(test_scores, axis=1), \"o-\", color=\"g\", label=\"test\")\n",
    "    plt.grid() ; plt.title(title) ; plt.legend(loc=\"best\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM (linear, rbf, poly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for kernel in ['linear', 'rbf', 'poly']:\n",
    "    svm_classifier = svm.SVC(kernel=kernel, probability=True, C=1, decision_function_shape='ovo') \n",
    "    plot_learning_curve(svm_classifier, f'{kernel} kernel', X_all_normal, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging ; logging.disable(logging.WARNING)\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "\n",
    "tf.random.set_seed(11235)\n",
    "\n",
    "nn_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(50, input_dim=X_train_normal.shape[1], activation='relu', kernel_initializer='he_uniform'),\n",
    "    tf.keras.layers.Dense(30, activation='relu', kernel_initializer='he_uniform'),\n",
    "    tf.keras.layers.Dense(20, activation='relu', kernel_initializer='he_uniform'),\n",
    "    tf.keras.layers.Dense(10, activation='relu', kernel_initializer='he_uniform'),\n",
    "    tf.keras.layers.Dense(len(np.unique(y)), activation='softmax', kernel_initializer='he_uniform')\n",
    "])\n",
    "\n",
    "model_metrics = ['sparse_categorical_accuracy', \n",
    "                 tf.metrics.SparseTopKCategoricalAccuracy(k=2)]\n",
    "                \n",
    "nn_model.compile(loss='sparse_categorical_crossentropy', \\\n",
    "                 #optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), \\\n",
    "                 optimizer='sgd', \\\n",
    "                 metrics=model_metrics, \\\n",
    "                 steps_per_execution=50)\n",
    "\n",
    "# history = nn_model.fit(X_train_normal, y_train, epochs=500, batch_size=256, verbose=2)\n",
    "plot_learning_curve(KerasClassifier(nn_model, batch_size=256, epochs=250), 'neural network', X_all_normal, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histogram Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient boosting score (accuracy)\t: 0.62375\n",
      "\tgradient boosting score (top-2)\t: 0.8671590909090909\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics         import top_k_accuracy_score\n",
    "from sklearn.experimental    import enable_hist_gradient_boosting\n",
    "from sklearn.model_selection import cross_val_score, RepeatedStratifiedKFold\n",
    "from sklearn.ensemble        import HistGradientBoostingClassifier\n",
    "\n",
    "hgb_classifier = HistGradientBoostingClassifier(max_bins=255, max_iter=100).fit(X_train_normal.toarray(), y_train)\n",
    "y_preds = hgb_classifier.predict_proba(X_test_normal.toarray())\n",
    "print(f'gradient boosting score (accuracy)\\t: {hgb_classifier.score(X_test_normal.toarray(), y_test)}')\n",
    "print(f'gradient boosting score (top-2)\\t: {top_k_accuracy_score(y_test, y_preds, k=2, labels=labels)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (confusion matrix display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import plot_confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# class estimator:\n",
    "#   _estimator_type = ''\n",
    "#   classes_=[]\n",
    "#   def __init__(self, model, classes):\n",
    "#     self.model = model\n",
    "#     self._estimator_type = 'classifier'\n",
    "#     self.classes_ = classes\n",
    "#   def predict(self, X):\n",
    "#     y_prob= self.model.predict(X)\n",
    "#     y_pred = y_prob.argmax(axis=1)\n",
    "#     return y_pred\n",
    "\n",
    "# classifier = estimator(nn_model, list(all_data_df['label'].unique()))\n",
    "\n",
    "# figsize = (12,12)\n",
    "# tmp = ConfusionMatrixDisplay.from_estimator(estimator=classifier, \n",
    "#                                             X=X_test_normal, \n",
    "#                                             y=y_test, \n",
    "#                                             cmap='Blues', \n",
    "#                                             normalize='true', \n",
    "#                                             ax=plt.subplots(figsize=figsize)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (old SVM training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn import svm\n",
    "\n",
    "# svm_classifier = svm.SVC(kernel='linear', probability=True, C=1, decision_function_shape='ovo').fit(X_train_normal, y_train)\n",
    "# y_preds = svm_classifier.predict_proba(X_test_normal)\n",
    "# print(f'linear score (accuracy)\\t: {svm_classifier.score(X_test_normal, y_test)}')\n",
    "# print(f'linear score (top-2)\\t: {top_k_accuracy_score(y_test, y_preds, k=2, labels=labels)}')\n",
    "# print('')\n",
    "\n",
    "# svm_classifier = svm.SVC(kernel='rbf',    probability=True, C=1, decision_function_shape='ovo').fit(X_train_normal, y_train)\n",
    "# y_preds = svm_classifier.predict_proba(X_test_normal)\n",
    "# print(f'rbf score (accuracy)\\t: {svm_classifier.score(X_test_normal, y_test)}')\n",
    "# print(f'linear score (top-2)\\t: {top_k_accuracy_score(y_test, y_preds, k=2, labels=labels)}')\n",
    "# print('')\n",
    "\n",
    "# svm_classifier = svm.SVC(kernel='poly',   probability=True, C=1, decision_function_shape='ovo').fit(X_train_normal, y_train)\n",
    "# y_preds = svm_classifier.predict_proba(X_test_normal)\n",
    "# print(f'poly score (accuracy)\\t: {svm_classifier.score(X_test_normal, y_test)}')\n",
    "# print(f'linear score (top-2)\\t: {top_k_accuracy_score(y_test, y_preds, k=2, labels=labels)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (old neural network accuracy graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(history.history['sparse_categorical_accuracy'][-1], '\\t', \\\n",
    "#       history.history['sparse_top_k_categorical_accuracy'][-1])\n",
    "\n",
    "# plt.rcParams.update({'figure.autolayout': True})\n",
    "# fig, axs = plt.subplots(2)\n",
    "# plt.xlabel('epoch')\n",
    "\n",
    "# axs[0].plot(history.history['sparse_top_k_categorical_accuracy'][3:])\n",
    "# axs[0].set_title(\"top-2 accuracy\")\n",
    "\n",
    "# axs[1].plot(history.history['loss'][3:])\n",
    "# axs[1].set_title(\"\\nloss\")\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
